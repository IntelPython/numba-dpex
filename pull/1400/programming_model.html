<!doctype html>
<html class="no-js" lang="en" data-content_root="./">
  <head><meta charset="utf-8"/>
    <meta name="viewport" content="width=device-width,initial-scale=1"/>
    <meta name="color-scheme" content="light dark"><meta name="viewport" content="width=device-width, initial-scale=1" />

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-LGGL0NJK6P"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-LGGL0NJK6P');
    </script>
    <link rel="index" title="Index" href="genindex.html" /><link rel="search" title="Search" href="search.html" /><link rel="next" title="Tutorials" href="user_guide/index.html" /><link rel="prev" title="Getting Started" href="getting_started.html" />

    <!-- Generated with Sphinx 7.2.6 and Furo 2024.01.29 -->
        <title>Programming Model - numba-dpex documentation</title>
      <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=a746c00c" />
    <link rel="stylesheet" type="text/css" href="_static/styles/furo.css?v=135e06be" />
    <link rel="stylesheet" type="text/css" href="_static/graphviz.css?v=eafc0fe6" />
    <link rel="stylesheet" type="text/css" href="_static/styles/furo-extensions.css?v=36a5483c" />
    <link rel="stylesheet" type="text/css" href="_static/css/custom.css" />
    
    


<style>
  body {
    --color-code-background: #f8f8f8;
  --color-code-foreground: black;
  
  }
  @media not print {
    body[data-theme="dark"] {
      --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
    }
    @media (prefers-color-scheme: dark) {
      body:not([data-theme="light"]) {
        --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
      }
    }
  }
</style></head>
  <body>
    
    <script>
      document.body.dataset.theme = localStorage.getItem("theme") || "auto";
    </script>
    

<svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
  <symbol id="svg-toc" viewBox="0 0 24 24">
    <title>Contents</title>
    <svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 1024 1024">
      <path d="M408 442h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8zm-8 204c0 4.4 3.6 8 8 8h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56zm504-486H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zm0 632H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zM115.4 518.9L271.7 642c5.8 4.6 14.4.5 14.4-6.9V388.9c0-7.4-8.5-11.5-14.4-6.9L115.4 505.1a8.74 8.74 0 0 0 0 13.8z"/>
    </svg>
  </symbol>
  <symbol id="svg-menu" viewBox="0 0 24 24">
    <title>Menu</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-menu">
      <line x1="3" y1="12" x2="21" y2="12"></line>
      <line x1="3" y1="6" x2="21" y2="6"></line>
      <line x1="3" y1="18" x2="21" y2="18"></line>
    </svg>
  </symbol>
  <symbol id="svg-arrow-right" viewBox="0 0 24 24">
    <title>Expand</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-chevron-right">
      <polyline points="9 18 15 12 9 6"></polyline>
    </svg>
  </symbol>
  <symbol id="svg-sun" viewBox="0 0 24 24">
    <title>Light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="feather-sun">
      <circle cx="12" cy="12" r="5"></circle>
      <line x1="12" y1="1" x2="12" y2="3"></line>
      <line x1="12" y1="21" x2="12" y2="23"></line>
      <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
      <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
      <line x1="1" y1="12" x2="3" y2="12"></line>
      <line x1="21" y1="12" x2="23" y2="12"></line>
      <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
      <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
    </svg>
  </symbol>
  <symbol id="svg-moon" viewBox="0 0 24 24">
    <title>Dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-moon">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M12 3c.132 0 .263 0 .393 0a7.5 7.5 0 0 0 7.92 12.446a9 9 0 1 1 -8.313 -12.454z" />
    </svg>
  </symbol>
  <symbol id="svg-sun-half" viewBox="0 0 24 24">
    <title>Auto light/dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-shadow">
      <path stroke="none" d="M0 0h24v24H0z" fill="none"/>
      <circle cx="12" cy="12" r="9" />
      <path d="M13 12h5" />
      <path d="M13 15h4" />
      <path d="M13 18h1" />
      <path d="M13 9h4" />
      <path d="M13 6h1" />
    </svg>
  </symbol>
</svg>

<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation">
<input type="checkbox" class="sidebar-toggle" name="__toc" id="__toc">
<label class="overlay sidebar-overlay" for="__navigation">
  <div class="visually-hidden">Hide navigation sidebar</div>
</label>
<label class="overlay toc-overlay" for="__toc">
  <div class="visually-hidden">Hide table of contents sidebar</div>
</label>



<div class="page">
  <header class="mobile-header">
    <div class="header-left">
      <label class="nav-overlay-icon" for="__navigation">
        <div class="visually-hidden">Toggle site navigation sidebar</div>
        <i class="icon"><svg><use href="#svg-menu"></use></svg></i>
      </label>
    </div>
    <div class="header-center">
      <a href="index.html"><div class="brand">numba-dpex  documentation</div></a>
    </div>
    <div class="header-right">
      <div class="theme-toggle-container theme-toggle-header">
        <button class="theme-toggle">
          <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
          <svg class="theme-icon-when-auto"><use href="#svg-sun-half"></use></svg>
          <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
          <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
        </button>
      </div>
      <label class="toc-overlay-icon toc-header-icon" for="__toc">
        <div class="visually-hidden">Toggle table of contents sidebar</div>
        <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
      </label>
    </div>
  </header>
  <aside class="sidebar-drawer">
    <div class="sidebar-container">
      
      <div class="sidebar-sticky"><a class="sidebar-brand" href="index.html">
  
  
  <span class="sidebar-brand-text">numba-dpex  documentation</span>
  
</a><form class="sidebar-search-container" method="get" action="search.html" role="search">
  <input class="sidebar-search" placeholder="Search" name="q" aria-label="Search">
  <input type="hidden" name="check_keywords" value="yes">
  <input type="hidden" name="area" value="default">
</form>
<div id="searchbox"></div><div class="sidebar-scroll"><div class="sidebar-tree">
  <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="overview.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="getting_started.html">Getting Started</a></li>
<li class="toctree-l1 current current-page"><a class="current reference internal" href="#">Programming Model</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="user_guide/index.html">Tutorials</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" role="switch" type="checkbox"/><label for="toctree-checkbox-1"><div class="visually-hidden">Toggle navigation of Tutorials</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="user_guide/kernel_programming/index.html">Kernel Programming</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="user_guide/debugging/index.html">Debugging with Intel® Distribution for GDB*</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" role="switch" type="checkbox"/><label for="toctree-checkbox-2"><div class="visually-hidden">Toggle navigation of Debugging with Intel® Distribution for GDB*</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="user_guide/debugging/set_up_machine.html">Set up the machine for debugging</a></li>
<li class="toctree-l3"><a class="reference internal" href="user_guide/debugging/debugging_environment.html">Configure debugging environment</a></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="user_guide/debugging/features.html">Supported Features</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" role="switch" type="checkbox"/><label for="toctree-checkbox-3"><div class="visually-hidden">Toggle navigation of Supported Features</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="user_guide/debugging/breakpoints.html">Breakpoints</a></li>
<li class="toctree-l4"><a class="reference internal" href="user_guide/debugging/stepping.html">Stepping</a></li>
<li class="toctree-l4"><a class="reference internal" href="user_guide/debugging/frame_info.html">Information About a Frame</a></li>
<li class="toctree-l4"><a class="reference internal" href="user_guide/debugging/backtrace.html">Backtrace</a></li>
<li class="toctree-l4"><a class="reference internal" href="user_guide/debugging/data.html">Examining Data</a></li>
<li class="toctree-l4"><a class="reference internal" href="user_guide/debugging/symbols.html">Examining the Symbol Table</a></li>
<li class="toctree-l4"><a class="reference internal" href="user_guide/debugging/altering.html">Altering Execution</a></li>
<li class="toctree-l4"><a class="reference internal" href="user_guide/debugging/local_variables.html">Debugging Local Variables</a></li>
<li class="toctree-l4"><a class="reference internal" href="user_guide/debugging/numba-0.55.html">Debugging Features in Numba 0.55</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="user_guide/debugging/limitations.html">Limitations</a></li>
<li class="toctree-l3"><a class="reference internal" href="user_guide/debugging/common_issues.html">Common issues and tips</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="user_guide/config.html">Configuration Options for <code class="docutils literal notranslate"><span class="pre">numba-dpex</span></code></a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="autoapi/index.html">API Reference</a><input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" role="switch" type="checkbox"/><label for="toctree-checkbox-4"><div class="visually-hidden">Toggle navigation of API Reference</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="autoapi/numba_dpex/kernel_api/index.html">numba_dpex.kernel_api</a></li>
<li class="toctree-l2"><a class="reference internal" href="autoapi/numba_dpex/core/decorators/index.html">numba_dpex.core.decorators</a></li>
<li class="toctree-l2"><a class="reference internal" href="autoapi/numba_dpex/core/kernel_launcher/index.html">numba_dpex.core.kernel_launcher</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="experimental/index.html">Experimental Features</a></li>
<li class="toctree-l1"><a class="reference internal" href="useful_links.html">Useful links</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Development</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="contribution_guide.html">Contribution Guide</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Misc Notes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="examples.html">List of examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="license.html">License</a></li>
<li class="toctree-l1"><a class="reference internal" href="release-notes.html">Release Notes</a></li>
</ul>

</div>
</div>

      </div>
      
    </div>
  </aside>
  <div class="main">
    <div class="content">
      <div class="article-container">
        <a href="#" class="back-to-top muted-link">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
            <path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12z"></path>
          </svg>
          <span>Back to top</span>
        </a>
        <div class="content-icon-container">
          
<div class="theme-toggle-container theme-toggle-content">
            <button class="theme-toggle">
              <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
              <svg class="theme-icon-when-auto"><use href="#svg-sun-half"></use></svg>
              <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
              <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
            </button>
          </div>
          <label class="toc-overlay-icon toc-content-icon" for="__toc">
            <div class="visually-hidden">Toggle table of contents sidebar</div>
            <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
          </label>
        </div>
        <article role="main">
          <span class="target" id="programming-model"></span><section id="id2">
<h1>Programming Model<a class="headerlink" href="#id2" title="Link to this heading">#</a></h1>
<p>In a heterogeneous system there may be <strong>multiple</strong> devices a Python user may
want to engage. For example, it is common for a consumer-grade laptop to feature
an integrated or a discrete GPU alongside a CPU.</p>
<p>To harness their power one needs to know how to answer the following 3 key
questions:</p>
<ol class="arabic simple">
<li><p>How does a Python program recognize available computational devices?</p></li>
<li><p>How does a Python workload specify computations to be offloaded to selected
devices?</p></li>
<li><p>How does a Python application manage data sharing?</p></li>
</ol>
<section id="recognizing-available-devices">
<h2>Recognizing available devices<a class="headerlink" href="#recognizing-available-devices" title="Link to this heading">#</a></h2>
<p>Python package <code class="docutils literal notranslate"><span class="pre">dpctl</span></code> answers these questions. All the computational devices
known to the underlying DPC++ runtime can be accessed using
<code class="docutils literal notranslate"><span class="pre">dpctl.get_devices()</span></code>. A specific device of interest <a class="reference external" href="https://intelpython.github.io/dpctl/latest/docfiles/user_guides/manual/dpctl/device_selection.html">can be selected</a>
either using a helper function, e.g. <code class="docutils literal notranslate"><span class="pre">dpctl.select_gpu_device()</span></code>, or by
passing a filter selector string to <code class="docutils literal notranslate"><span class="pre">dpctl.SyclDevice</span></code> constructor.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">dpctl</span>

<span class="c1"># select a GPU device. If multiple devices present,</span>
<span class="c1"># let the underlying runtime select from GPUs</span>
<span class="n">dev_gpu</span> <span class="o">=</span> <span class="n">dpctl</span><span class="o">.</span><span class="n">SyclDevice</span><span class="p">(</span><span class="s2">&quot;gpu&quot;</span><span class="p">)</span>
<span class="c1"># select a CPU device</span>
<span class="n">dev_cpu</span> <span class="o">=</span> <span class="n">dpctl</span><span class="o">.</span><span class="n">SyclDevice</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">)</span>

<span class="c1"># stand-alone function, equivalent to C++</span>
<span class="c1">#   `auto dev = sycl::gpu_selector().select_device();`</span>
<span class="n">dev_gpu_alt</span> <span class="o">=</span> <span class="n">dpctl</span><span class="o">.</span><span class="n">select_gpu_device</span><span class="p">()</span>
<span class="c1"># stand-alone function, equivalent to C++</span>
<span class="c1">#   `auto dev = sycl::cpu_selector().select_device();`</span>
<span class="n">dev_cpu_alt</span> <span class="o">=</span> <span class="n">dpctl</span><span class="o">.</span><span class="n">select_cpu_device</span><span class="p">()</span>
</pre></div>
</div>
<p>A <a class="reference external" href="https://intelpython.github.io/dpctl/latest/docfiles/user_guides/manual/dpctl/devices.html">device object</a>
can be used to query properies of the device, such as its name, vendor, maximal
number of computational units, memory size, etc.</p>
</section>
<section id="specifying-offload-target">
<h2>Specifying offload target<a class="headerlink" href="#specifying-offload-target" title="Link to this heading">#</a></h2>
<p>To answer the second question on the list we need a digression to explain
offloading in oneAPI DPC++ first.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In DPC++, a computation kernel can be specified using generic C++
programming and then the kernel can be offloaded to any device that is
supported by an underlying SYCL runtime. The device to which the kernel
is offloaded is specified using an <strong>execution queue</strong> when <em>launching
the kernel</em>.</p>
<p>The oneAPI unified programming model brings portability across heterogeneous
architectures. Another important aspect of the programming model is its
inherent flexibility that makes it possible to go beyond portability and even
strive for performance portability. An oneAPI library may be implemented
using C++ techniques such as template metaprogramming or dynamic polymorphism
to implement specializations for a generic kernel. If a kernel is implemented
polymorphically, the specialized implementation will be dispatched based on
the execution queue specified during kernel launch. The oneMKL library is an
example of a performance portable oneAPI library.</p>
</div>
<p>A computational task is offloaded for execution on a device by submitting it to
DPC++ runtime which inserts the task in a computational graph. Once the device
becomes available the runtime selects a task whose dependencies are met for
execution. The computational graph as well as the device targeted by its tasks
are stored in a <a class="reference external" href="https://intelpython.github.io/dpctl/latest/docfiles/user_guides/manual/dpctl/queues.html">SYCL queue</a>
object. The task submission is therefore always associated with a queue.</p>
<p>Queues can be constructed directly from a device object, or by using a filter
selector string to indicate the device to construct:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># construct queue from device object</span>
<span class="n">q1</span> <span class="o">=</span> <span class="n">dpctl</span><span class="o">.</span><span class="n">SyclQueue</span><span class="p">(</span><span class="n">dev_gpu</span><span class="p">)</span>
<span class="c1"># construct queue using filter selector</span>
<span class="n">q2</span> <span class="o">=</span> <span class="n">dpctl</span><span class="o">.</span><span class="n">SyclQueue</span><span class="p">(</span><span class="s2">&quot;gpu&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>The computational tasks can be stored in an oneAPI native extension in which
case their submission is orchestrated during Python API calls. Let’s consider a
function that offloads an evaluation of a polynomial for every point of a NumPy
array <code class="docutils literal notranslate"><span class="pre">X</span></code>. Such a function needs to receive a queue object to indicate which
device the computation must be offloaded to:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># allocate space for the result</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty_like</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="c1"># evaluate polynomial on the device targeted by the queue, Y[i] = p(X[i])</span>
<span class="n">onapi_ext</span><span class="o">.</span><span class="n">offloaded_poly_evaluate</span><span class="p">(</span><span class="n">exec_q</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
</pre></div>
</div>
<p>Python call to <code class="docutils literal notranslate"><span class="pre">onapi_ext.offloaded_poly_evaluate</span></code> applied to NumPy arrays of
double precision floating pointer numbers gets translated to the following
sample C++ code:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="kt">void</span>
<span class="nf">cpp_offloaded_poly_evaluate</span><span class="p">(</span>
<span class="w">  </span><span class="n">sycl</span><span class="o">::</span><span class="n">queue</span><span class="w"> </span><span class="n">q</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="kt">double</span><span class="w"> </span><span class="o">*</span><span class="n">X</span><span class="p">,</span><span class="w"> </span><span class="kt">double</span><span class="w"> </span><span class="o">*</span><span class="n">Y</span><span class="p">,</span><span class="w"> </span><span class="kt">size_t</span><span class="w"> </span><span class="n">n</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="c1">// create buffers from malloc allocations to make data accessible from device</span>
<span class="w">    </span><span class="n">sycl</span><span class="o">::</span><span class="n">buffer</span><span class="o">&lt;</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="kt">double</span><span class="o">&gt;</span><span class="w"> </span><span class="n">buf_X</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="w"> </span><span class="n">n</span><span class="p">);</span>
<span class="w">    </span><span class="n">sycl</span><span class="o">::</span><span class="n">buffer</span><span class="o">&lt;</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="kt">double</span><span class="o">&gt;</span><span class="w"> </span><span class="n">buf_Y</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span><span class="w"> </span><span class="n">n</span><span class="p">);</span>

<span class="w">    </span><span class="n">q</span><span class="p">.</span><span class="n">submit</span><span class="p">([</span><span class="o">&amp;</span><span class="p">](</span><span class="n">sycl</span><span class="o">::</span><span class="n">handler</span><span class="w"> </span><span class="o">&amp;</span><span class="n">cgh</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="c1">// create buffer accessors indicating kernel data-flow pattern</span>
<span class="w">        </span><span class="n">sycl</span><span class="o">::</span><span class="n">accessor</span><span class="w"> </span><span class="n">acc_X</span><span class="p">(</span><span class="n">buf_X</span><span class="p">,</span><span class="w"> </span><span class="n">cgh</span><span class="p">,</span><span class="w"> </span><span class="n">sycl</span><span class="o">::</span><span class="n">read_only</span><span class="p">);</span>
<span class="w">        </span><span class="n">sycl</span><span class="o">::</span><span class="n">accessor</span><span class="w"> </span><span class="n">acc_Y</span><span class="p">(</span><span class="n">buf_Y</span><span class="p">,</span><span class="w"> </span><span class="n">cgh</span><span class="p">,</span><span class="w"> </span><span class="n">sycl</span><span class="o">::</span><span class="n">write_only</span><span class="p">,</span><span class="w"> </span><span class="n">sycl</span><span class="o">::</span><span class="n">no_init</span><span class="p">);</span>

<span class="w">        </span><span class="n">cgh</span><span class="p">.</span><span class="n">parallel_for</span><span class="p">(</span><span class="n">n</span><span class="p">,</span>
<span class="w">           </span><span class="c1">// lambda function that gets executed by different work-items with</span>
<span class="w">           </span><span class="c1">// different arguments in parallel</span>
<span class="w">           </span><span class="p">[</span><span class="o">=</span><span class="p">](</span><span class="n">sycl</span><span class="o">::</span><span class="n">id</span><span class="o">&lt;</span><span class="mi">1</span><span class="o">&gt;</span><span class="w"> </span><span class="n">id</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">              </span><span class="k">auto</span><span class="w"> </span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">accX</span><span class="p">[</span><span class="n">id</span><span class="p">];</span>
<span class="w">              </span><span class="n">accY</span><span class="p">[</span><span class="n">id</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mf">3.0</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">x</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="p">(</span><span class="mf">1.0</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">x</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="p">(</span><span class="mf">-0.5</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mf">0.3</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">x</span><span class="p">));</span>
<span class="w">           </span><span class="p">});</span>
<span class="w">    </span><span class="p">}).</span><span class="n">wait</span><span class="p">();</span>

<span class="w">    </span><span class="k">return</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
<p>We refer an interested reader to an excellent and freely available “<a class="reference external" href="https://link.springer.com/book/10.1007%2F978-1-4842-5574-2">Data
Parallel C++</a>”
book for details of this data parallel C++.</p>
<p>Our package <code class="docutils literal notranslate"><span class="pre">numba_dpex</span></code> allows one to write kernels directly in Python.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numba_dpex</span>


<span class="nd">@numba_dpex</span><span class="o">.</span><span class="n">kernel</span>
<span class="k">def</span> <span class="nf">numba_dpex_poly</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">):</span>
    <span class="n">i</span> <span class="o">=</span> <span class="n">numba_dpex</span><span class="o">.</span><span class="n">get_global_id</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="n">Y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="mf">3.0</span> <span class="o">+</span> <span class="n">x</span> <span class="o">*</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">+</span> <span class="n">x</span> <span class="o">*</span> <span class="p">(</span><span class="o">-</span><span class="mf">0.5</span> <span class="o">+</span> <span class="mf">0.3</span> <span class="o">*</span> <span class="n">x</span><span class="p">))</span>
</pre></div>
</div>
<p>Specifying the execution queue is done using Python context manager:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10</span><span class="o">**</span><span class="mi">6</span><span class="p">)</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty_like</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="k">with</span> <span class="n">dpctl</span><span class="o">.</span><span class="n">device_context</span><span class="p">(</span><span class="n">q</span><span class="p">):</span>
    <span class="c1"># apply the kernel to elements of X, writing value into Y,</span>
    <span class="c1"># while executing using given queue</span>
    <span class="n">numba_dpex_poly</span><span class="p">[</span><span class="n">numba_dpex</span><span class="o">.</span><span class="n">Range</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">size</span><span class="p">)](</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
</pre></div>
</div>
<p>The argument to <code class="docutils literal notranslate"><span class="pre">device_context</span></code> can be a queue object, a device object for
which a temporary queue will be created, or a filter selector string. Thus we
could have equally used <code class="docutils literal notranslate"><span class="pre">dpctl.device_context(gpu_dev)</span></code> or
<code class="docutils literal notranslate"><span class="pre">dpctl.device_context(&quot;gpu&quot;)</span></code>.</p>
<p>Note that in this examples data sharing was implicitly managed for us: in the
case of calling a function from a precompiled oneAPI native extension data
sharing was managed by DPC++ runtime, while in the case of using <code class="docutils literal notranslate"><span class="pre">numba_dpex</span></code>
kernel it was managed during execution of <code class="docutils literal notranslate"><span class="pre">__call__</span></code> method.</p>
</section>
<section id="data-sharing">
<h2>Data sharing<a class="headerlink" href="#data-sharing" title="Link to this heading">#</a></h2>
<p>Implicit management of data is surely convenient, but its use in an interpreted
code comes at a performance cost. A runtime must implicitly copy data from host
to the device before the kernel execution commences and then copy some (or all)
of it back after the execution completes for every Python API call.</p>
<p><code class="docutils literal notranslate"><span class="pre">dpctl</span></code> provides for allocating memory directly accessible to kernels
executing on a device using SYCL’s Unified Shared Memory (<a class="reference external" href="https://www.khronos.org/registry/SYCL/specs/sycl-2020/html/sycl-2020.html#sec:usm">USM</a>)
feature. It also implements USM-based ND-array object
<code class="docutils literal notranslate"><span class="pre">dpctl.tensor.usm_ndarray</span></code> that conforms <a class="reference external" href="https://data-apis.org/array-api/latest/">array-API standard</a>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">dpctl.tensor</span> <span class="k">as</span> <span class="nn">dpt</span>

<span class="c1"># allocate array of doubles using USM-device allocation on GPU device</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">dpt</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s2">&quot;gpu&quot;</span><span class="p">,</span> <span class="n">usm_type</span><span class="o">=</span><span class="s2">&quot;device&quot;</span><span class="p">)</span>
<span class="c1"># allocate array for the output</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">dpt</span><span class="o">.</span><span class="n">empty_like</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="c1"># execution queue is inferred from allocation queues.</span>
<span class="c1"># Kernel is executed on the same device where arrays were allocated</span>
<span class="n">numba_dpex_poly</span><span class="p">[</span><span class="n">X</span><span class="o">.</span><span class="n">size</span><span class="p">,</span> <span class="n">numba_dpex</span><span class="o">.</span><span class="n">DEFAULT_LOCAL_SIZE</span><span class="p">](</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
</pre></div>
</div>
<p>The execution queue can be unambiguously determined in this case since both
arguments are USM arrays with the same allocation queues and <code class="docutils literal notranslate"><span class="pre">X.sycl_queue</span> <span class="pre">==</span>
<span class="pre">Y.sycl_queue</span></code> evaluates to <code class="docutils literal notranslate"><span class="pre">True</span></code>. Should allocation queues be different,
such an inference becomes ambiguous and <code class="docutils literal notranslate"><span class="pre">numba_dpex</span></code> raises
<code class="docutils literal notranslate"><span class="pre">IndeterminateExecutionQueueError</span></code> advising user to explicitly migrate the
data.</p>
<p>Migration can be accomplished either by using <code class="docutils literal notranslate"><span class="pre">dpctl.tensor.asarray(X,</span>
<span class="pre">device=target_device)</span></code> to create a copy, or by using
<code class="docutils literal notranslate"><span class="pre">X.to_device(target_device)</span></code> method.</p>
<p>A USM array can be copied back into a NumPy array using <code class="docutils literal notranslate"><span class="pre">dpt.asnumpy(Y)</span></code> if
needed.</p>
</section>
<section id="compute-follows-data">
<h2>Compute follows data<a class="headerlink" href="#compute-follows-data" title="Link to this heading">#</a></h2>
<p>Automatic deduction of the execution queue from allocation queues is consitent
with “<a class="reference external" href="https://data-apis.org/array-api/latest/design_topics/device_support.html">local control for data allocation target</a>”
in the array API standard. User has full control over memory allocation through
three keyword arguments present in all <a class="reference external" href="https://data-apis.org/array-api/latest/API_specification/creation_functions.html">array creation functions</a>.
For example, consider</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># TODO</span>
</pre></div>
</div>
<p>The keyword <code class="docutils literal notranslate"><span class="pre">device</span></code> is <a class="reference external" href="https://data-apis.org/array-api/latest/design_topics/device_support.html#syntax-for-device-assignment">mandated by the array API</a>.
In <code class="docutils literal notranslate"><span class="pre">dpctl.tensor</span></code> the allowed values of the keyword are</p>
<ul class="simple">
<li><p>Filter selector string, e.g. <code class="docutils literal notranslate"><span class="pre">device=&quot;gpu:0&quot;</span></code></p></li>
<li><p>Existing <code class="docutils literal notranslate"><span class="pre">dpctl.SyclDevice</span></code> object, e.g. <code class="docutils literal notranslate"><span class="pre">device=dev_gpu</span></code></p></li>
<li><p>Existing <code class="docutils literal notranslate"><span class="pre">dpctl.SyclQueue</span></code> object</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">dpctl.tensor.Device</span></code> object instance obtained from an existing USM array,
e.g. <code class="docutils literal notranslate"><span class="pre">device=X.device</span></code></p></li>
</ul>
<p>In all cases, an allocation queue object will be constructed as described
<a class="reference external" href="#specifying-offload-target">earlier</a> and stored in the array instance,
accessible with <code class="docutils literal notranslate"><span class="pre">X.sycl_queue</span></code>. Instead of using <code class="docutils literal notranslate"><span class="pre">device</span></code> keyword, one can
alternatively use <code class="docutils literal notranslate"><span class="pre">sycl_queue</span></code> keyword for readability to directly specify a
<code class="docutils literal notranslate"><span class="pre">dpctl.SyclQueue</span></code> object to be used as the allocation queue.</p>
<p>The rationale for storing the allocation queue in the array is that kernels
submitted to this queue are guaranteed to be able to correctly dereference (i.e.
access) the USM pointer. Array operations that only involve this single USM
array can thus execute on the allocation queue, and the output array can be
allocated on this same allocation queue with the same usm type as the input
array.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Reusing the allocation queue of the input
array ensures the computational tasks behind the API call can access the
array without making implicit copies and the output array is allocated
on the same device as the input.</p>
</div>
<p>Compute follows data is the rule prescribing deduction of the execution and the
allocation queue as well as the USM type for the result when multiple USM arrays
are combined. It stipulates that arrays can be combined if and only if their
allocation <em>queues are the same</em> as measured by <code class="docutils literal notranslate"><span class="pre">==</span></code> operator (i.e.
<code class="docutils literal notranslate"><span class="pre">X.sycl_queue</span> <span class="pre">==</span> <span class="pre">Y.sycl_queue</span></code> must evaluate to <code class="docutils literal notranslate"><span class="pre">True</span></code>). Same queues refer
to the same underlying task graphs and DPC++ schedulers.</p>
<p>An attempt to combine USM arrays with unsame allocation queues raises an
exception advising the user to migrate the data. Migration can be accomplished
either by using <code class="docutils literal notranslate"><span class="pre">dpctl.tensor.asarray(X,</span> <span class="pre">device=Y.device)</span></code> to create a copy,
or by using <code class="docutils literal notranslate"><span class="pre">X.to_device(Y.device)</span></code> method which can sometime do the migration
more efficiently.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p><code class="docutils literal notranslate"><span class="pre">dpctl</span></code> and <code class="docutils literal notranslate"><span class="pre">numba_dpex</span></code> are both under heavy development. Feel free to file an
issue on GitHub or reach out on Gitter should you encounter any issues.</p>
</div>
</section>
</section>

        </article>
      </div>
      <footer>
        
        <div class="related-pages">
          <a class="next-page" href="user_guide/index.html">
              <div class="page-info">
                <div class="context">
                  <span>Next</span>
                </div>
                <div class="title">Tutorials</div>
              </div>
              <svg class="furo-related-icon"><use href="#svg-arrow-right"></use></svg>
            </a>
          <a class="prev-page" href="getting_started.html">
              <svg class="furo-related-icon"><use href="#svg-arrow-right"></use></svg>
              <div class="page-info">
                <div class="context">
                  <span>Previous</span>
                </div>
                
                <div class="title">Getting Started</div>
                
              </div>
            </a>
        </div>
        <div class="bottom-of-page">
          <div class="left-details">
            <div class="copyright">
                Copyright &#169; 2020-2024, Intel Corporation
            </div>
            Made with <a href="https://www.sphinx-doc.org/">Sphinx</a> and <a class="muted-link" href="https://pradyunsg.me">@pradyunsg</a>'s
            
            <a href="https://github.com/pradyunsg/furo">Furo</a>
            
          </div>
          <div class="right-details">
            <div class="icons">
              <a class="muted-link " href="https://github.com/IntelPython/numba-dpex" aria-label="GitHub"></a>
              
            </div>
          </div>
        </div>
        
      </footer>
    </div>
    <aside class="toc-drawer">
      
      
      <div class="toc-sticky toc-scroll">
        <div class="toc-title-container">
          <span class="toc-title">
            On this page
          </span>
        </div>
        <div class="toc-tree-container">
          <div class="toc-tree">
            <ul>
<li><a class="reference internal" href="#">Programming Model</a><ul>
<li><a class="reference internal" href="#recognizing-available-devices">Recognizing available devices</a></li>
<li><a class="reference internal" href="#specifying-offload-target">Specifying offload target</a></li>
<li><a class="reference internal" href="#data-sharing">Data sharing</a></li>
<li><a class="reference internal" href="#compute-follows-data">Compute follows data</a></li>
</ul>
</li>
</ul>

          </div>
        </div>
      </div>
      
      
    </aside>
  </div>
</div><script src="_static/documentation_options.js?v=5929fcd5"></script>
    <script src="_static/doctools.js?v=888ff710"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/scripts/furo.js?v=32e29ea5"></script>
    </body>
</html>