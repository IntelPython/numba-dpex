.. _index:
.. include:: ./../../ext_links.txt

Kernel Programming
##################

The tutorial covers the numba-dpex kernel programming API (kapi) and introduces
the concepts needed to write data-parallel kernels in numba-dpex.


.. Preliminary concepts
.. --------------------

.. Data parallelism
.. ++++++++++++++++

.. Single Program Multiple Data
.. ++++++++++++++++++++++++++++

.. Range v/s Nd-Range Kernels
.. ++++++++++++++++++++++++++

.. Work items and Work groups
.. ++++++++++++++++++++++++++

Core concepts
*************


Writing a *range* kernel
========================

.. include:: ./writing-range-kernel.rst

Writing an *nd-range* kernel
============================

In a range kernel, the kernel execution is scheduled over a set of work-items
without any explicit grouping of the work-items. The basic form of parallelism
that can be expressed using a range kernel does not allow expressing any notion
of locality within the kernel. To get around that limitation, kapi provides a
second form of expressing a parallel kernel that is called an *nd-range* kernel.
An nd-range kernel represents a data-parallel execution of the kernel by a set
of explicitly defined groups of work-items. An individual group of work-items is
called a *work-group*. :ref:`ex_matmul_kernel` demonstrates an nd-range kernel
and some of the advanced features programmers can use in this type of kernel.

.. code-block:: python
    :linenos:
    :caption: **Example:** Sliding window matrix multiplication as an nd-range kernel
    :name: ex_matmul_kernel

    from numba_dpex import kernel_api as kapi
    import numba_dpex.experimental as dpex_exp
    import numpy as np
    import dpctl.tensor as dpt

    square_block_side = 2
    work_group_size = (square_block_side, square_block_side)
    dtype = np.float32


    @dpex_exp.kernel
    def matmul(
        nditem: kapi.NdItem,
        X,  # IN READ-ONLY    (X_n_rows, n_cols)
        y,  # IN READ-ONLY    (n_cols, y_n_rows),
        X_slm,  # SLM to store a sliding window over X
        Y_slm,  # SLM to store a sliding window over Y
        result,  # OUT        (X_n_rows, y_n_rows)
    ):
        X_n_rows = X.shape[0]
        Y_n_cols = y.shape[1]
        n_cols = X.shape[1]

        result_row_idx = nditem.get_global_id(0)
        result_col_idx = nditem.get_global_id(1)

        local_row_idx = nditem.get_local_id(0)
        local_col_idx = nditem.get_local_id(1)

        n_blocks_for_cols = n_cols // square_block_side
        if (n_cols % square_block_side) > 0:
            n_blocks_for_cols += 1

        output = dtype(0)

        gr = nditem.get_group()

        for block_idx in range(n_blocks_for_cols):
            X_slm[local_row_idx, local_col_idx] = dtype(0)
            Y_slm[local_row_idx, local_col_idx] = dtype(0)
            if (result_row_idx < X_n_rows) and (
                (local_col_idx + (square_block_side * block_idx)) < n_cols
            ):
                X_slm[local_row_idx, local_col_idx] = X[
                    result_row_idx, local_col_idx + (square_block_side * block_idx)
                ]

            if (result_col_idx < Y_n_cols) and (
                (local_row_idx + (square_block_side * block_idx)) < n_cols
            ):
                Y_slm[local_row_idx, local_col_idx] = y[
                    local_row_idx + (square_block_side * block_idx), result_col_idx
                ]

            kapi.group_barrier(gr)

            for idx in range(square_block_side):
                output += X_slm[local_row_idx, idx] * Y_slm[idx, local_col_idx]

            kapi.group_barrier(gr)

        if (result_row_idx < X_n_rows) and (result_col_idx < Y_n_cols):
            result[result_row_idx, result_col_idx] = output


    def _arange_reshaped(shape, dtype):
        n_items = shape[0] * shape[1]
        return np.arange(n_items, dtype=dtype).reshape(shape)


    X = _arange_reshaped((5, 5), dtype)
    Y = _arange_reshaped((5, 5), dtype)
    X = dpt.asarray(X)
    Y = dpt.asarray(Y)
    device = X.device.sycl_device
    result = dpt.zeros((5, 5), dtype, device=device)
    X_slm = kapi.LocalAccessor(shape=work_group_size, dtype=dtype)
    Y_slm = kapi.LocalAccessor(shape=work_group_size, dtype=dtype)

    dpex_exp.call_kernel(matmul, kapi.NdRange((6, 6), (2, 2)), X, Y, X_slm, Y_slm, result)


When writing an nd-range kernel, a programmer
defines a set of groups of work-items instead of a flat execution range

An nd-range kernel needs to be launched with
an instance of the :py:class:`numba_dpex.kernel_api.NdRange` class and the first
argument to an nd-range kernel has to be an instance of
:py:class:`numba_dpex.kernel_api.NdItem`. An ``NdRange`` object defines a set of
work-groups each with it own set of work-items.

.. Launching a kernel
.. ==================

.. include:: ./call-kernel.rst

The ``device_func`` decorator
=============================

.. include:: ./device-functions.rst


Supported types of kernel argument
==================================

A kapi kernel function can have both array and scalar arguments. At least one of
the argument to every kernel function has to be an array. The requirement is
enforced so that a execution queue can be inferred at the kernel launch stage.
An array type argument is passed as a reference to the kernel and all scalar
arguments are passed by value.

Supported array types
---------------------
- `dpctl.tensor.usm_ndarray`_ : A SYCL-based Python Array API complaint tensor.
- `dpnp.ndarray`_ :  A ``numpy.ndarray`` type container that supports SYCL USM memory allocation.

Scalar types
------------

Scalar values can be passed to a kernel function either using the default Python
scalar type or as explicit NumPy or dpnp data type objects.
:ref:`ex_scalar_kernel_arg_ty` shows the two possible ways of defining a scalar
type. In both scenarios, numba-dpex depends on the default Numba* type inferring
algorithm to determine the LLVM IR type of a Python object that represents a
scalar value. At the kernel submission stage the LLVM IR type is reinterpreted
as a C++11 type to interoperate with the underlying SYCL runtime.

.. code-block:: python
    :caption: **Example:** Ways of defining a scalar kernel argument
    :name: ex_scalar_kernel_arg_ty

    import dpnp

    a = 1
    b = dpnp.dtype("int32").type(1)

    print(type(a))
    print(type(b))

.. code-block:: bash
    :caption: **Output:** Ways of defining a scalar kernel argument
    :name: ex_scalar_kernel_arg_ty_output

    <class 'int'>
    <class 'numpy.int32'>

The following scalar types are currently supported as arguments of a numba-dpex
kernel function:

- ``int``
- ``float``
- ``complex``
- ``numpy.int32``
- ``numpy.uint32``
- ``numpy.int64``
- ``numpy.uint32``
- ``numpy.float32``
- ``numpy.float64``

.. important::

    The Numba* type inferring algorithm by default infers a native Python
    scalar type to be a 64-bit value. The algorithm is defined that way to be
    consistent with the default CPython behavior. The default inferred 64-bit
    type can cause compilation failures on platforms that do not have native
    64-bit floating point support. Another potential fallout of the default
    64-bit type inference can be when a narrower width type is required by a
    specific kernel. To avoid these issues, users are advised to always use a
    dpnp/numpy type object to explicitly define the type of a scalar value.

DLPack support
--------------
At this time direct support for the `DLPack`_ protocol is has not been added to
numba-dpex. To interoperate numba_dpex with other SYCL USM based libraries,
users should first convert their input tensor or ndarray object into either of
the two supported array types, both of which support DLPack.


Supported Python features
*************************

Mathematical operations
=======================

.. include:: ./math-functions.rst

Operators
=========

General Python features
=======================

.. include:: ./supported-python-features.rst


Advanced concepts
*****************

Local memory allocation
=======================

Private memory allocation
=========================

Group barrier synchronization
=============================

Atomic operations
=================

.. Async kernel execution
.. ======================

.. include:: ./call-kernel-async.rst

Specializing a kernel or a device_func
======================================
